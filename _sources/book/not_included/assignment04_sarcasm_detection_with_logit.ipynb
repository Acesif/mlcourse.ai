{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b05e9ba",
   "metadata": {},
   "source": [
    "(assignment04_extra)=\n",
    "\n",
    "\n",
    "# Assignment 4 (demo). Sarcasm detection with logistic regression\n",
    "\n",
    "\n",
    "```{figure} /_static/img/ods_stickers.jpg\n",
    "```\n",
    "\n",
    "Author: [Yury Kashnitsky](https://www.linkedin.com/in/festline/). All content is distributed under the [Creative Commons CC BY-NC-SA 4.0](https://creativecommons.org/licenses/by-nc-sa/4.0/) license.\n",
    "\n",
    "\n",
    "**Same assignment as a [Kaggle Notebook](https://www.kaggle.com/kashnitsky/a4-demo-sarcasm-detection-with-logit) + [solution](https://www.kaggle.com/kashnitsky/a4-demo-sarcasm-detection-with-logit-solution).**\n",
    "\n",
    "\n",
    "We'll be using the dataset from the [paper](https://arxiv.org/abs/1704.05579) \"A Large Self-Annotated Corpus for Sarcasm\" with >1mln comments from Reddit, labeled as either sarcastic or not. A processed version can be found on Kaggle in a form of a [Kaggle Dataset](https://www.kaggle.com/danofer/sarcasm).\n",
    "\n",
    "Sarcasm detection is easy.\n",
    "\n",
    "\n",
    "```{figure} /_static/img/yeah.jpeg\n",
    ":width: 400px\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3796ab15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# some necessary imports\n",
    "import os\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from matplotlib import pyplot as plt\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ceafec2d",
   "metadata": {},
   "source": [
    "The dataset is a bit too large to be stored on GitHub. So, we download it from Google Drive. Alternatively, you can download the \"train-balanced-sarcasm.csv.zip\" file from [Kaggle](https://www.kaggle.com/danofer/sarcasm?select=train-balanced-sarcasm.csv.zip) ([alternative link](https://drive.google.com/file/d/1KbBdJaEY8RF4GXzoihWgH0RdqoVBZ_oi/view?usp=sharing)) and place the file in a convenient place, then modify the `DATA_PATH` below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f546d5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def download_file_from_gdrive(file_url, filename, out_path='../../_static', overwrite=False):\n",
    "    \"\"\"\n",
    "    Downloads a file from GDrive given an URL\n",
    "    :param file_url: a string formated as https://drive.google.com/uc?id=<file_id>\n",
    "    :param: the desired file name\n",
    "    :param: the desired folder where the file will be downloaded to\n",
    "    :param overwrite: whether to overwrite the file if it already exists\n",
    "    \"\"\"\n",
    "    file_exists = os.path.exists(f'{out_path}/{filename}')\n",
    "\n",
    "    if (file_exists and overwrite) or (not file_exists):\n",
    "    \tos.system(f'gdown {file_url} -O {out_path}/{filename}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c96f8b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_URL = 'https://drive.google.com/uc?id=1KbBdJaEY8RF4GXzoihWgH0RdqoVBZ_oi'\n",
    "FILE_NAME = 'train-balanced-sarcasm.csv.zip'\n",
    "DATA_PATH = '../../_static/data/'\n",
    "\n",
    "download_file_from_gdrive(file_url=FILE_URL, filename= FILE_NAME, out_path=DATA_PATH)\n",
    "\n",
    "train_df = pd.read_csv(DATA_PATH + \"train-balanced-sarcasm.csv.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00ae4fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f43f32",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4805ec3",
   "metadata": {},
   "source": [
    "Some comments are missing, so we drop the corresponding rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a656b0f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(subset=[\"comment\"], inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2773e943",
   "metadata": {},
   "source": [
    "We notice that the dataset is indeed balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b44abdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df[\"label\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16c5b38c",
   "metadata": {},
   "source": [
    "We split data into training and validation parts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c4d1b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts, valid_texts, y_train, y_valid = train_test_split(\n",
    "    train_df[\"comment\"], train_df[\"label\"], random_state=17\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a8d1ce1",
   "metadata": {},
   "source": [
    "## Tasks:\n",
    "1. Analyze the dataset, make some plots. This [Kernel](https://www.kaggle.com/sudalairajkumar/simple-exploration-notebook-qiqc) might serve as an example\n",
    "2. Build a Tf-Idf + logistic regression pipeline to predict sarcasm (`label`) based on the text of a comment on Reddit (`comment`).\n",
    "3. Plot the words/bigrams which a most predictive of sarcasm (you can use [eli5](https://github.com/TeamHG-Memex/eli5) for that)\n",
    "4. (optionally) add subreddits as new features to improve model performance. Apply here the Bag of Words approach, i.e. treat each subreddit as a new feature.\n",
    "\n",
    "## Links:\n",
    "  - Machine learning library [Scikit-learn](https://scikit-learn.org/stable/index.html) (a.k.a. sklearn)\n",
    "  - Kernels on [logistic regression](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-2-classification) and its applications to [text classification](https://www.kaggle.com/kashnitsky/topic-4-linear-models-part-4-more-of-logit), also a [Kernel](https://www.kaggle.com/kashnitsky/topic-6-feature-engineering-and-feature-selection) on feature engineering and feature selection\n",
    "  - [Kaggle Kernel](https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle) \"Approaching (Almost) Any NLP Problem on Kaggle\"\n",
    "  - [ELI5](https://github.com/TeamHG-Memex/eli5) to explain model predictions"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "md:myst",
   "text_representation": {
    "extension": ".md",
    "format_name": "myst"
   }
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "source_map": [
   11,
   38,
   51,
   55,
   70,
   80,
   85,
   87,
   92,
   94,
   99,
   101,
   106,
   110
  ]
 },
 "nbformat": 4,
 "nbformat_minor": 5
}